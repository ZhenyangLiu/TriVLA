<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TriVLA: A Triple-System based Unified Vision-Language-Action Model for General Robot Control">
  <meta name="keywords" content="Vision Language Action, Policy Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TriVLA: A Triple-System based Unified Vision-Language-Action Model for General Robot Control</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">TriVLA: A Triple-System based Unified Vision-Language-Action Model for General Robot Control</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhenyangliu.github.io/">Zhenyang Liu</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Yongchong Gu</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Sixiao Zheng</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="">Xiangyang Xue</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Yanwei Fu</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute,</span>
          </div>
          <p class="has-text-weight-bold has-text-dark is-size-4">CVPR 2025</p>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 使用 Flexbox 居中 -->
      <div class="has-text-centered">
        <img src="./static/images/teaser.png" alt="Teaser Image" style="width: 70%; height: auto;">
      </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">ReasonGrounder</span> achieves the task of open-vocabulary 3D visual grounding and reasoning. In a given scene, the user observes from a perspective with occlusions and asks questions such as: ``Can you localize the red, round, sweet fruit on the table that is partially occluded by the toy sheep?" Open-vocabulary 3D visual grounding and reasoning seeks to interpret complex implicit queries, deduce answers, and accurately localize the target object, even when it is partially or fully occluded from the current viewpoint.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Open-vocabulary 3D visual grounding and reasoning aim to localize objects in a scene based on implicit language descriptions, even when they are occluded. This ability is crucial for tasks such as vision-language navigation and autonomous robotics. However, current methods struggle because they rely heavily on fine-tuning with 3D annotations and mask proposals, which limits their ability to handle diverse semantics and common knowledge required for effective reasoning.
          </p>
          <p>
          To address this, we propose ReasonGrounder, an LVLM-guided framework that uses hierarchical 3D feature Gaussian fields for adaptive grouping based on physical scale, enabling open-vocabulary 3D grounding and reasoning. ReasonGrounder interprets implicit instructions using large vision-language models (LVLM) and localizes occluded objects through 3D Gaussian splatting.
          By incorporating 2D segmentation masks from the Segment Anything Model (SAM) and multi-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on object scale, enabling accurate localization through both explicit and implicit language understanding, even in novel, occluded views.
          </p>
          <p>
          We also contribute ReasoningGD, a new dataset containing over 10K scenes and 2 million annotations for evaluating open-vocabulary 3D grounding and amodal perception under occlusion. Experiments show that ReasonGrounder improves 3D grounding accuracy in real-world scenarios.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
        </p>
          <!-- 图片居中 -->
          <div class="has-text-centered">
            <img src="./static/images/introduction.png" alt="Introduction Image" style="width: 100%; height: auto;">
          </div>
          <p>
          ReasonGrounder employs 3D Gaussian Splatting (3DGS), which represents scenes as 3D Gaussian collections with tile-based splatting for efficient, high-resolution rendering. In particular, a standard 3DGS scene is constructed, and 2D segmentation masks from SAM~\cite{kirillov2023segment} are projected into a 3D field. For each mask, a 3D scale is calculated from the depth rendered by the 3DGS. To enhance each Gaussian’s view-independent representation, ReasonGrounder appends a latent feature vector mapped into hierarchical language and instance features via two shallow MLPs: a language mapper and an instance mapper. CLIP embeddings supervise language features across views for multi-view consistency, while instance features refine 2D mask candidates using contrastive loss and 3D scale, supporting feature-based Gaussian grouping.
          Further, to aid localization, an instruction-conditioned mechanism guided by LVLM selects the reference view most aligned with the instruction. This view and instruction enable comprehension of the target object of intent. Using 3D scale and hierarchical feature Gaussians, ReasonGrounder achieves precise 3D localization and amodal perception in novel views.          </p>
          <p>
          </p>
          <h4 class="subtitle is-4">Main Contributions:</h4>
          <ul class="content">
            <li>Enhanced 3D Visual Grounding: We address limitations in open-vocabulary 3D visual grounding, such as reliance on 3D annotations and limited semantic understanding, by using hierarchical 3D Gaussian fields with LVLMs for robust grounding and reasoning.</li>
            <li>A Novel ReasonGrounder Framework: The proposed ReasonGrounder leverages hierarchical 3D feature Gaussian fields for adaptive Gaussian grouping with 3D scale, enabling effective open-vocabulary 3D visual grounding and reasoning. ReasonGrounder interprets implicit instructions using large vision-language models (LVLM) and accurately localizes occluded objects with hierarchical 3D feature Gaussian splatting.</li>
            <li>Hierarchical Feature Splatting and Amodal Perception: ReasonGrounder empowers the hierarchical features of 3D Gaussians and selects Gaussian groups based on the target object's scale. LVLM aids in interpreting complex instructions and locating objects even when partially or fully occluded.</li>
            <li>Dataset Contributions: A new ReasoningGD dataset offers over 10K complex scenes with 2 million annotations, including point clouds, RGB-D images with detailed labels, camera poses, and 2D modal/amodal masks of views, enabling rigorous evaluation of 3D visual grounding with implicit instruction handling and occlusion robustness.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <!-- 图片居中 -->
          <div class="has-text-centered">
            <img src="./static/images/qual_1.png" alt="Introduction Image" style="width: 95%; height: auto;">
          </div>
          <div class="has-text-centered">
            <img src="./static/images/qual_2.png" alt="Introduction Image" style="width: 95%; height: auto;">
          </div>
          <ul class="content">
            <li> ReasonGrounder uses LVLM to process implicit queries requiring complex reasoning and select the Gaussian group for 3D localization. The results confirm that ReasonGrounder excels in localizing target objects, benefiting from the 3D consistency. Additionally, ReasonGrounder provides explanatory answers, showcasing its strength in implicit instruction reasoning, 3D understanding, and conversation. To test robustness, we selected five challenging scenes with small proportions, including multi-hierarchical structures and similar objects, along with ten text queries per scene from the LERF and ReasoningGD datasets.</li>
            <li>Existing open-vocabulary 3D visual grounding methods struggle with localizing complete objects in novel views with occlusion, limiting their real-world applicability. In contrast, ReasonGrounder uses hierarchical Gaussian grouping to effectively tackle this issue. The ReasoningGD dataset introduced here includes scenes with amodal binary mask annotations, accurately representing the full shape of occluded objects from different views. These results demonstrate that ReasonGrounder successfully achieves amodal perception, accurately localizing complete objects regardless of the occlusion level.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ReasoningGD dataset</h2>
        <div class="content has-text-justified">
          <!-- 图片居中 -->
          <div class="has-text-centered">
            <img src="./static/images/dataset.png" alt="Introduction Image" style="width: 95%; height: auto;">
          </div>
          <p>
        This paper introduces the ReasoningGD dataset, which encompasses a diverse range of occlusion scenarios and offers comprehensive, accurate annotations. These annotations include both the visible and occluded parts of target objects from various perspectives, enabling more robust evaluation of reasoning capabilities in 3D visual grounding. The dataset comprises over 10K scenes, each featuring 10 to 15 objects. Each scene includes 100 viewing angles, with annotations provided for both the visible mask of each object at each angle and the full mask, which includes occluded parts. In total, the dataset contains over 2 million detailed annotations.    </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Theme modified from 
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noopener noreferrer">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
